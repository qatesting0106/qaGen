# GenAI QA System API Reference

## Overview

This API reference provides detailed information about the classes and methods available in the GenAI QA System. It is intended for developers who want to use the system programmatically or extend its functionality.

## Core Components

### QAProcessor

The `QAProcessor` class handles document loading, embedding creation, vector store building, and answer generation using RAG pipeline.

```python
class QAProcessor:
    def __init__(self, mistral_api_key: str, groq_api_key: str, chunk_size: int = 1000, chunk_overlap: int = 200)
```

**Parameters:**
- `mistral_api_key` (str): API key for Mistral AI embeddings
- `groq_api_key` (str): API key for Groq language model
- `chunk_size` (int, optional): Size of document chunks. Default is 1000
- `chunk_overlap` (int, optional): Overlap between chunks. Default is 200

#### Methods

```python
def load_documents(self, file_path: str, file_type: str = None) -> List[Document]
```

**Description:** Load and split documents from the given file path.

**Parameters:**
- `file_path` (str): Path to the document file
- `file_type` (str, optional): Type of file ('txt', 'pdf', 'docx'). If None, inferred from extension

**Returns:**
- List[Document]: List of document chunks with metadata

**Raises:**
- `FileNotFoundError`: If the file does not exist
- `ValueError`: If file type is not supported
- `Exception`: If there is an error loading the document

---

```python
def initialize_embeddings(self, model_name: str = "mistral-embed") -> None
```

**Description:** Initialize the Mistral AI embeddings model.

**Parameters:**
- `model_name` (str, optional): Name of the embedding model. Default is "mistral-embed"

**Returns:** None

**Raises:**
- `ValueError`: If model initialization fails
- `AuthenticationError`: If API key is invalid

---

```python
def create_vector_store(self, documents: List[Document], store_path: str = None) -> None
```

**Description:** Create FAISS vector store from documents.

**Parameters:**
- `documents` (List[Document]): List of document chunks
- `store_path` (str, optional): Path to save the vector store. If None, stored in memory

**Returns:** None

**Raises:**
- `ValueError`: If documents list is empty
- `Exception`: If vector store creation fails

---

```python
def generate_answer(self, question: str, max_context_length: int = 2000) -> Dict[str, Any]
```

**Description:** Generate answer for a given question using the RAG pipeline.

**Parameters:**
- `question` (str): The question to answer
- `max_context_length` (int, optional): Maximum length of context to include. Default is 2000

**Returns:**
- Dict[str, Any]: Dictionary containing:
  - 'answer' (str): Generated answer
  - 'context' (List[str]): Relevant context chunks
  - 'sources' (List[Dict]): Source documents with metadata
  - 'confidence' (float): Confidence score of the answer

**Raises:**
- `ValueError`: If question is empty or invalid
- `Exception`: If answer generation fails

### MetricsEvaluator

The `MetricsEvaluator` class calculates various performance metrics for generated answers.

```python
class MetricsEvaluator:
    def __init__(self, output_dir: str, metrics_config: Dict[str, bool] = None)
```

**Parameters:**
- `output_dir` (str): Directory to store evaluation results
- `metrics_config` (Dict[str, bool], optional): Configuration for which metrics to calculate

#### Methods

```python
def calculate_metrics(self, generated_answer: str, reference_context: str, question: str) -> Dict[str, float]
```

**Description:** Calculate various metrics for the generated answer.

**Parameters:**
- `generated_answer` (str): The answer generated by the system
- `reference_context` (str): The reference context
- `question` (str): The original question

**Returns:**
- Dict[str, float]: Dictionary containing:
  - 'context_precision' (float): Precision of context retrieval
  - 'context_recall' (float): Recall of context retrieval
  - 'context_f1' (float): F1 score of context retrieval
  - 'answer_relevance' (float): Relevance score of answer
  - 'answer_completeness' (float): Completeness score of answer
  - 'answer_consistency' (float): Consistency score with context
  - 'cosine_similarity' (float): Cosine similarity with reference
  - 'faithfulness' (float): Faithfulness score of answer

**Raises:**
- `ValueError`: If any input parameter is empty or invalid
- `Exception`: If metrics calculation fails

### SecurityEvaluator

The `SecurityEvaluator` class implements OWASP LLM security guidelines for comprehensive evaluation.

```python
class SecurityEvaluator:
    def __init__(self, risk_threshold: float = 0.7)
```

**Parameters:**
- `risk_threshold` (float, optional): Threshold for risk assessment. Default is 0.7

#### Methods

```python
def evaluate_security(self, generated_answer: str, prompt: str) -> Dict[str, Any]
```

**Description:** Perform security assessment of generated answer.

**Parameters:**
- `generated_answer` (str): The answer generated by the system
- `prompt` (str): The original prompt/question

**Returns:**
- Dict[str, Any]: Dictionary containing:
  - 'prompt_injection_risk' (float): Risk score for prompt injection
  - 'information_disclosure_risk' (float): Risk score for information disclosure
  - 'output_validation_score' (float): Score for output validation
  - 'agency_risk_score' (float): Score for agency risk
  - 'overall_risk_score' (float): Overall security risk score
  - 'recommendations' (List[str]): Security recommendations

**Raises:**
- `ValueError`: If input parameters are invalid
- `Exception`: If security evaluation fails

**Returns:**
- Dictionary containing calculated metrics

### DashboardComponents

The `DashboardComponents` class provides UI elements for the Streamlit dashboard.

```python
class DashboardComponents:
    @staticmethod
    def display_metrics_summary(metrics_df: pd.DataFrame) -> None
```

**Description:** Display individual metrics for each response.

**Parameters:**
- `metrics_df` (pd.DataFrame): DataFrame containing metrics data

```python
    @staticmethod
    def plot_metrics_comparison(metrics_df: pd.DataFrame) -> None
```

**Description:** Plot comparison of different metrics.

**Parameters:**
- `metrics_df` (pd.DataFrame): DataFrame containing metrics data

```python
    @staticmethod
    def plot_metrics_heatmap(metrics_df: pd.DataFrame) -> None
```

**Description:** Plot correlation heatmap of metrics.

**Parameters:**
- `metrics_df` (pd.DataFrame): DataFrame containing metrics data