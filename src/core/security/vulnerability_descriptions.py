from typing import Dict, Any

class VulnerabilityDescriptions:
    """
    Provides detailed descriptions and mitigation strategies for OWASP LLM vulnerabilities.
    This class contains comprehensive information about each vulnerability category
    in the OWASP LLM Top 10 framework.
    """
    
    # Detailed descriptions of each OWASP LLM vulnerability category
    DESCRIPTIONS = {
        'LLM01': {
            'title': 'Prompt Injection',
            'description': 'Prompt injection occurs when an attacker manipulates an LLM through crafted inputs '
                          'that cause the model to ignore previous instructions or perform unintended actions. '
                          'This can lead to the model generating harmful content, revealing sensitive information, '
                          'or bypassing security controls.',
            'impact': 'High - Can lead to complete system compromise, data leakage, or malicious outputs.',
            'examples': [
                'Inputs containing "ignore previous instructions"',
                'Messages that attempt to override system prompts',
                'Jailbreaking attempts using special characters or formatting'
            ],
            'mitigations': [
                'Implement input validation and sanitization',
                'Use defensive prompt engineering techniques',
                'Apply content filtering on both inputs and outputs',
                'Implement a robust permission model for sensitive operations'
            ]
        },
        'LLM02': {
            'title': 'Insecure Output Handling',
            'description': 'Occurs when LLM-generated content is not properly validated or sanitized before being '
                          'processed or displayed. This can lead to injection attacks like XSS, SQL injection, '
                          'or command injection when the output is used in sensitive contexts.',
            'impact': 'Critical - Can lead to client-side attacks, data breaches, or server compromise.',
            'examples': [
                'LLM generates JavaScript code with malicious content',
                'Output contains HTML/script tags that execute in the browser',
                'Generated SQL queries with injection payloads'
            ],
            'mitigations': [
                'Implement strict output validation and sanitization',
                'Use content security policies',
                'Apply context-specific encoding (HTML, SQL, shell)',
                'Never execute LLM-generated code without review'
            ]
        },
        'LLM03': {
            'title': 'Training Data Poisoning',
            'description': 'Occurs when an attacker manipulates the training data or fine-tuning process to '
                          'introduce vulnerabilities or biases into the model. This can create backdoors or '
                          'cause the model to generate harmful content in response to specific triggers.',
            'impact': 'High - Long-term compromise of model behavior and security.',
            'examples': [
                'Injecting malicious examples during fine-tuning',
                'Poisoning public datasets used for training',
                'Creating adversarial examples that trigger specific behaviors'
            ],
            'mitigations': [
                'Validate and clean training datasets',
                'Implement robust data governance processes',
                'Monitor model behavior for unexpected patterns',
                'Use adversarial training techniques'
            ]
        },
        'LLM04': {
            'title': 'Model Stealing',
            'description': 'Occurs when an attacker extracts information about the model\'s architecture, '
                          'parameters, or training data through carefully crafted queries. This can lead to '
                          'intellectual property theft or enable more sophisticated attacks.',
            'impact': 'Medium - Loss of competitive advantage and potential security compromise.',
            'examples': [
                'Systematic querying to extract model parameters',
                'Attempts to reconstruct training data',
                'Probing for model architecture details'
            ],
            'mitigations': [
                'Implement rate limiting and usage monitoring',
                'Use query filtering to detect extraction attempts',
                'Apply differential privacy techniques',
                'Monitor for unusual usage patterns'
            ]
        },
        'LLM05': {
            'title': 'Sensitive Information Disclosure',
            'description': 'Occurs when an LLM inadvertently reveals confidential information, either from its '
                          'training data or from user interactions. This can include personal data, credentials, '
                          'or proprietary information.',
            'impact': 'Critical - Data breaches, privacy violations, and compliance issues.',
            'examples': [
                'Model reveals API keys or credentials in responses',
                'Disclosure of personal information from training data',
                'Leaking internal system details or configurations'
            ],
            'mitigations': [
                'Implement data minimization principles',
                'Use output filtering for sensitive patterns',
                'Apply redaction techniques for high-risk content',
                'Regular security audits and red team testing'
            ]
        },
        'LLM06': {
            'title': 'Denial of Service',
            'description': 'Occurs when an attacker exploits the computational intensity of LLMs to consume '
                          'excessive resources, causing service degradation or outages. This can be achieved '
                          'through crafted inputs that trigger resource-intensive processing.',
            'impact': 'Medium - Service disruption and increased operational costs.',
            'examples': [
                'Inputs designed to maximize token usage',
                'Requests that trigger recursive or complex reasoning',
                'Crafted prompts that cause excessive computation'
            ],
            'mitigations': [
                'Implement request quotas and rate limiting',
                'Set maximum token limits for inputs and outputs',
                'Monitor resource usage patterns',
                'Use tiered access controls'
            ]
        },
        'LLM07': {
            'title': 'Training Data Extraction',
            'description': 'Occurs when an attacker is able to extract portions of the model\'s training data '
                          'through carefully crafted queries. This can lead to privacy violations and '
                          'intellectual property theft.',
            'impact': 'High - Privacy violations and potential legal/compliance issues.',
            'examples': [
                'Prompt techniques that cause verbatim recall of training data',
                'Extraction of copyrighted content',
                'Retrieval of personal information from training corpus'
            ],
            'mitigations': [
                'Apply memorization detection techniques',
                'Implement output filtering for known training data',
                'Use differential privacy during training',
                'Regular auditing for data extraction vulnerabilities'
            ]
        },
        'LLM08': {
            'title': 'Model Manipulation',
            'description': 'Occurs when an attacker manipulates the model\'s behavior through adversarial '
                          'techniques, causing it to produce biased, harmful, or incorrect outputs. This can '
                          'undermine the model\'s reliability and safety.',
            'impact': 'High - Compromised model integrity and potential harm to users.',
            'examples': [
                'Adversarial examples that cause misclassification',
                'Inputs designed to trigger specific biases',
                'Manipulation of model context to alter reasoning'
            ],
            'mitigations': [
                'Implement robust adversarial training',
                'Use ensemble approaches for critical decisions',
                'Apply input validation and sanitization',
                'Regular red team testing'
            ]
        },
        'LLM09': {
            'title': 'Supply Chain Attacks',
            'description': 'Occurs when an attacker compromises the LLM development pipeline, pre-trained models, '
                          'or third-party components. This can introduce backdoors or vulnerabilities that are '
                          'difficult to detect.',
            'impact': 'Critical - Widespread and persistent compromise.',
            'examples': [
                'Compromised pre-trained models with backdoors',
                'Malicious code in model libraries or dependencies',
                'Tampering with model weights during distribution'
            ],
            'mitigations': [
                'Verify integrity of pre-trained models',
                'Implement secure supply chain practices',
                'Regular security audits of dependencies',
                'Use trusted sources for models and components'
            ]
        },
        'LLM10': {
            'title': 'Excessive Agency',
            'description': 'Occurs when an LLM is given or assumes too much autonomy in decision-making or '
                          'actions. This can lead to unintended consequences when the model makes decisions '
                          'beyond its capabilities or authority.',
            'impact': 'Medium - Unintended actions and potential harm to users.',
            'examples': [
                'Model initiating actions without user confirmation',
                'Autonomous decision-making in sensitive contexts',
                'Overconfident responses in areas of uncertainty'
            ],
            'mitigations': [
                'Implement clear boundaries for model agency',
                'Require human approval for consequential actions',
                'Design systems with appropriate levels of autonomy',
                'Regular testing of agency limitations'
            ]
        },
        'LLM11': {
            'title': 'Vector Embedding Weaknesses',
            'description': 'Occurs when an attacker exploits vulnerabilities in the vector embedding space of an LLM. '
                          'This can include embedding poisoning, adversarial examples targeting the embedding space, '
                          'or extraction of sensitive information from embeddings.',
            'impact': 'High - Can lead to model manipulation, data leakage, and security bypasses.',
            'examples': [
                'Adversarial perturbations that cause embedding misclassification',
                'Embedding inversion attacks to extract training data',
                'Poisoning attacks targeting the embedding space',
                'Extraction of sensitive information from embedding vectors'
            ],
            'mitigations': [
                'Implement adversarial training for embedding robustness',
                'Apply differential privacy to embedding vectors',
                'Use dimensionality reduction techniques to limit information leakage',
                'Regular auditing of embedding space for anomalies'
            ]
        },
        'LLM12': {
            'title': 'Misinformation Generation',
            'description': 'Occurs when an LLM generates false, misleading, or fabricated information that appears '
                          'credible. This can lead to the spread of misinformation, harm to users relying on the '
                          'information, and damage to trust in AI systems.',
            'impact': 'Critical - Can cause widespread harm through dissemination of false information.',
            'examples': [
                'Generation of fabricated facts or statistics',
                'Creation of misleading content without uncertainty indicators',
                'Hallucination of non-existent sources or references',
                'Presenting opinions as factual statements'
            ],
            'mitigations': [
                'Implement fact-checking mechanisms for generated content',
                'Use source attribution for factual statements',
                'Train models to express appropriate uncertainty',
                'Implement content filtering for known misinformation patterns'
            ]
        },
        'LLM13': {
            'title': 'Unbounded Consumption',
            'description': 'Occurs when an LLM consumes excessive computational resources, either through '
                          'inefficient processing or as a result of malicious inputs designed to trigger '
                          'resource-intensive operations. This can lead to service degradation, increased costs, '
                          'or complete system unavailability.',
            'impact': 'High - Can cause system outages, performance degradation, and increased operational costs.',
            'examples': [
                'Inputs designed to trigger exponential computational complexity',
                'Prompts that cause excessive token generation',
                'Requests that lead to memory leaks or resource exhaustion',
                'Crafted inputs that bypass resource limits'
            ],
            'mitigations': [
                'Implement strict resource limits and timeouts',
                'Monitor and throttle resource usage',
                'Use efficient algorithms and optimizations',
                'Implement progressive resource allocation based on trust'
            ]
        }
    }
    
    @classmethod
    def get_description(cls, category_id: str) -> Dict[str, Any]:
        """Get the detailed description for a specific vulnerability category."""
        return cls.DESCRIPTIONS.get(category_id, {
            'title': 'Unknown Vulnerability',
            'description': 'No description available for this vulnerability category.',
            'impact': 'Unknown',
            'examples': [],
            'mitigations': []
        })
    
    @classmethod
    def get_all_descriptions(cls) -> Dict[str, Dict[str, Any]]:
        """Get all vulnerability descriptions."""
        return cls.DESCRIPTIONS